{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4fa8f253-b2a5-4d71-8734-98f711881408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A B C D "
     ]
    }
   ],
   "source": [
    "# Recursive depth first search\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def dfs_recursive(graph, visited, node):\n",
    "    if node not in visited: \n",
    "        print(node, end=' ')\n",
    "        visited.add(node)\n",
    "        for neighbor in graph[node]:\n",
    "            dfs_recursive(graph, visited, neighbor)\n",
    "\n",
    "df = pd.read_csv(\"graph.csv\", index_col=0)\n",
    "graph = df.apply(lambda row: [col for col in df.columns if row[col] == 1], axis=1).to_dict()\n",
    "visited = set()\n",
    "start = list(graph.keys())[0]\n",
    "dfs_recursive(graph, visited, start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a3f6f2c3-f828-4dde-ad89-f2e0a01d8f04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A B C D \n",
      " {'A': ['B', 'D'], 'B': ['A', 'C'], 'C': ['B', 'D'], 'D': ['A', 'C']}\n"
     ]
    }
   ],
   "source": [
    "# Non recursive depth first search\n",
    "\n",
    "def non_recursive(node):\n",
    "    visited = set()\n",
    "    stack = [node]\n",
    "    while stack:\n",
    "        n = stack.pop()\n",
    "        if n not in visited:\n",
    "            print(n, end=' ')\n",
    "            visited.add(n)\n",
    "            stack.extend(reversed(graph[n]))\n",
    "            \n",
    "non_recursive(start)\n",
    "print(\"\\n\",graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8dc2e946-8d48-45d8-a991-98096f58d55b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A B D C "
     ]
    }
   ],
   "source": [
    "# Breadth first search\n",
    "\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "def bfs(graph,start):\n",
    "    visited = set()\n",
    "    queue = deque([start])\n",
    "\n",
    "    while queue:\n",
    "        node = queue.popleft()\n",
    "        if node not in visited:\n",
    "            print(node, end= ' ')\n",
    "            visited.add(node)\n",
    "            queue.extend(graph[node])\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "bfs(graph, 'A')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "cfdbd55d-b9a2-4b14-a4b2-770c90f89de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A C B D "
     ]
    }
   ],
   "source": [
    "# Best First Search\n",
    "import heapq\n",
    "\n",
    "def best_first_search(start, end):\n",
    "    visited = set()\n",
    "    queue = [(heuristics[start], start)]\n",
    "    while queue: \n",
    "        _, node = heapq.heappop(queue)\n",
    "        if node not in visited:\n",
    "            print(node, end=' ')\n",
    "            visited.add(node)\n",
    "            if node == end:\n",
    "                break\n",
    "            for neighbor in graph[node]:\n",
    "                if neighbor not in visited:\n",
    "                    heapq.heappush(queue, (heuristics[neighbor], neighbor))\n",
    "                \n",
    "df = pd.read_csv(\"graph.csv\", index_col=0)\n",
    "graph = df.apply(lambda row: [col for col in df.columns if row[col] == 1], axis=1).to_dict()\n",
    "graph = {\n",
    "    'A': ['B','C'],\n",
    "    'B': ['C','D'],\n",
    "    'C': [],\n",
    "    'D':[]\n",
    "}\n",
    "heuristics = {'A': 3, 'B': 2,'C': 1,'D': 0}\n",
    "best_first_search(start, 'D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f29fc2bc-3a2d-442f-a6ff-01c6190e67d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A C D "
     ]
    }
   ],
   "source": [
    "# Best first search weighted heuristics\n",
    "\n",
    "def best_first_search_weighted(start, goal):\n",
    "    visited = set()\n",
    "    queue = [(heuristics[start],start)]\n",
    "    while queue:\n",
    "        _, node = heapq.heappop(queue)\n",
    "        if node not in visited:\n",
    "            print(node, end=' ')\n",
    "            visited.add(node) \n",
    "            if node == goal:\n",
    "                break\n",
    "            for neighbor, weight in graph[node]:\n",
    "                heapq.heappush(queue, (heuristics[neighbor], neighbor))\n",
    "\n",
    "graph = {\n",
    "    'A': [('B', 1), ('C', 3)],\n",
    "    'B': [('A', 1), ('D', 2)],\n",
    "    'C': [('A', 3), ('D', 1)],\n",
    "    'D': [('B', 2), ('C', 1)]\n",
    "}\n",
    "heuristics = {'A': 3, 'B': 2, 'C': 1, 'D': 0}\n",
    "best_first_search_weighted( 'A', 'D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "040ac9d6-8a71-4c08-98b5-dfc378401c76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A B D C G \n",
      "Reached G with total cost of 8\n"
     ]
    }
   ],
   "source": [
    "# A* Algorithm\n",
    "import pandas as pd\n",
    "import heapq\n",
    "\n",
    "def read_from_csv(file):\n",
    "    df = pd.read_csv(file)\n",
    "    graph = {}\n",
    "    heuristics = {}\n",
    "\n",
    "    for _,row in df.iterrows():\n",
    "        src = row['source']\n",
    "        dest = row['dest']\n",
    "        weight = int(row['weight'])\n",
    "        heuristic = int(row['heuristic'])\n",
    "\n",
    "        if src not in graph:\n",
    "            graph[src] = []\n",
    "        graph[src].append((dest,weight))\n",
    "\n",
    "        heuristics[src] = heuristic\n",
    "\n",
    "        if dest not in heuristics:\n",
    "            heuristics[dest] = 0\n",
    "    return graph, heuristics\n",
    "\n",
    "def a_star(start, goal):\n",
    "    visited = set()\n",
    "    que = [(heuristics[start],0,start)]\n",
    "\n",
    "    while que:\n",
    "        f,cost,node = heapq.heappop(que)\n",
    "        if node==goal:\n",
    "            print(node, end=' ')\n",
    "            print(f\"\\nReached {node} with total cost of {cost}\")\n",
    "            return\n",
    "        if node not in visited:\n",
    "            print(node, end=' ')\n",
    "            visited.add(node)\n",
    "            for neighbor,weight in graph.get(node, []):\n",
    "                g= cost + weight\n",
    "                h = heuristics.get(neighbor, 0)\n",
    "                heapq.heappush(que,(g+h,g,neighbor))\n",
    "                \n",
    "\n",
    "\n",
    "graph, heuristics = read_from_csv(\"a_star_graph.csv\")\n",
    "\n",
    "a_star('A','G')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d20bfac6-b948-4ceb-9093-7703baebabd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Union: {'x': 0.7, 'y': 0.5, 'z': 0.9}\n",
      "Intersection: {'x': 0.2, 'y': 0.1, 'z': 0.6}\n",
      "Complement of A: {'x': 0.8, 'y': 0.5, 'z': 0.19999999999999996}\n"
     ]
    }
   ],
   "source": [
    "A = {'x': 0.2, 'y': 0.5, 'z': 0.8}\n",
    "B = {'x': 0.4, 'y': 0.3, 'z': 0.9}\n",
    "C = {'x': 0.7, 'y': 0.1, 'z': 0.6}\n",
    "\n",
    "def union(*sets):\n",
    "    return {k: max(s.get(k, 0) for s in sets) for k in sets[0]}\n",
    "\n",
    "def intersection(*sets):\n",
    "    return {k: min(s.get(k, 0) for s in sets) for k in sets[0]}\n",
    "\n",
    "def complement(s):\n",
    "    return {k: 1 - v for k, v in s.items()}\n",
    "\n",
    "print(\"Union:\", union(A, B, C))\n",
    "print(\"Intersection:\", intersection(A, B, C))\n",
    "print(\"Complement of A:\", complement(A))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "62b086b0-2f24-42f2-b602-89f497c92cdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¬(A ∪ B): {'x': 0.4, 'y': 0.30000000000000004}\n",
      "¬A ∩ ¬B: {'x': 0.4, 'y': 0.30000000000000004}\n"
     ]
    }
   ],
   "source": [
    "A = {'x': 0.6, 'y': 0.2}\n",
    "B = {'x': 0.4, 'y': 0.7}\n",
    "\n",
    "union_set = {k: max(A[k], B[k]) for k in A}\n",
    "comp_union = {k: 1 - union_set[k] for k in union_set}\n",
    "\n",
    "comp_A = {k: 1 - A[k] for k in A}\n",
    "comp_B = {k: 1 - B[k] for k in B}\n",
    "intersection_comp = {k: min(comp_A[k], comp_B[k]) for k in comp_A}\n",
    "\n",
    "print(\"¬(A ∪ B):\", comp_union)\n",
    "print(\"¬A ∩ ¬B:\", intersection_comp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d9e3384b-c4b9-4d08-856d-70f80c3d1217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter bricks:(-1 to stop)  2\n",
      "Enter bricks:(-1 to stop)  3\n",
      "Enter bricks:(-1 to stop)  4\n",
      "Enter bricks:(-1 to stop)  -1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Result (1=Win, -1=Lose): 1\n"
     ]
    }
   ],
   "source": [
    "def is_game_over(heap):\n",
    "    return all(pile == 0 for pile in heap)\n",
    "\n",
    "def evaluate(heap):\n",
    "    return 1 if sum(heap) == 0 else 0\n",
    "\n",
    "def minimax(heap, is_maximizing):\n",
    "    if is_game_over(heap):\n",
    "        return -1 if is_maximizing else 1\n",
    "\n",
    "    if is_maximizing:\n",
    "        best = float('-inf')\n",
    "        for i in range(len(heap)):\n",
    "            for remove in range(1, heap[i] + 1):\n",
    "                new_heap = heap[:]\n",
    "                new_heap[i] -= remove\n",
    "                best = max(best, minimax(new_heap, False))\n",
    "        return best\n",
    "    else:\n",
    "        best = float('inf')\n",
    "        for i in range(len(heap)):\n",
    "            for remove in range(1, heap[i] + 1):\n",
    "                new_heap = heap[:]\n",
    "                new_heap[i] -= remove\n",
    "                best = min(best, minimax(new_heap, True))\n",
    "        return best\n",
    "\n",
    "heap = []\n",
    "while True:\n",
    "    inp = int(input(\"Enter bricks:(-1 to stop) \"))\n",
    "    if inp==-1:\n",
    "        break\n",
    "    heap.append(inp)\n",
    "print(\"Optimal Result (1=Win, -1=Lose):\", minimax(heap, True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "1d68dc51-ba1b-4c36-9503-58593a3459d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to Nim! Piles: [5, 5, 5]\n",
      "\n",
      "Your turn. Current piles: [5, 5, 5]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " Choose pile index (0–2):  1\n",
      " Remove how many stones from pile 1?  4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computer removes 1 from pile 0. New piles: [4, 1, 5]\n",
      "\n",
      "Your turn. Current piles: [4, 1, 5]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " Choose pile index (0–2):  0\n",
      " Remove how many stones from pile 0?  3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computer removes 5 from pile 2. New piles: [1, 1, 0]\n",
      "\n",
      "Your turn. Current piles: [1, 1, 0]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " Choose pile index (0–2):  0\n",
      " Remove how many stones from pile 0?  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computer removes 1 from pile 1. New piles: [0, 0, 0]\n",
      "\n",
      "Game over!\n",
      "💻 Computer took the last stone — you lose!\n"
     ]
    }
   ],
   "source": [
    "import heapq\n",
    "\n",
    "def is_game_over(heap):\n",
    "    return all(pile == 0 for pile in heap)\n",
    "\n",
    "def minimax(heap, is_maximizing):\n",
    "    # Base case: no moves left\n",
    "    if is_game_over(heap):\n",
    "        return -1 if is_maximizing else 1\n",
    "\n",
    "    if is_maximizing:\n",
    "        best = float('-inf')\n",
    "        for i in range(len(heap)):\n",
    "            for remove in range(1, heap[i] + 1):\n",
    "                new_heap = heap.copy()\n",
    "                new_heap[i] -= remove\n",
    "                score = minimax(new_heap, False)\n",
    "                best = max(best, score)\n",
    "                # Alpha-beta style short-circuit:\n",
    "                if best == 1:\n",
    "                    return 1\n",
    "        return best\n",
    "    else:\n",
    "        best = float('inf')\n",
    "        for i in range(len(heap)):\n",
    "            for remove in range(1, heap[i] + 1):\n",
    "                new_heap = heap.copy()\n",
    "                new_heap[i] -= remove\n",
    "                score = minimax(new_heap, True)\n",
    "                best = min(best, score)\n",
    "                if best == -1:\n",
    "                    return -1\n",
    "        return best\n",
    "\n",
    "def get_computer_move(heap):\n",
    "    \"\"\"Choose the move giving the highest minimax score for the computer.\"\"\"\n",
    "    best_score = float('-inf')\n",
    "    best_move = None\n",
    "    for i in range(len(heap)):\n",
    "        for remove in range(1, heap[i] + 1):\n",
    "            new_heap = heap.copy()\n",
    "            new_heap[i] -= remove\n",
    "            score = minimax(new_heap, False)  # Next is human's turn\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_move = (i, remove)\n",
    "            # If you can force a win, pick immediately\n",
    "            if best_score == 1:\n",
    "                return best_move\n",
    "    return best_move\n",
    "\n",
    "# --- Game loop ---\n",
    "heap = [5, 5, 5]    # initial piles\n",
    "current_player = 'human'  # human starts\n",
    "last_player = None\n",
    "\n",
    "print(\"Welcome to Nim! Piles:\", heap)\n",
    "while not is_game_over(heap):\n",
    "    if current_player == 'human':\n",
    "        print(\"\\nYour turn. Current piles:\", heap)\n",
    "        # get valid human move\n",
    "        while True:\n",
    "            i = int(input(f\" Choose pile index (0–{len(heap)-1}): \"))\n",
    "            r = int(input(f\" Remove how many stones from pile {i}? \"))\n",
    "            if 0 <= i < len(heap) and 1 <= r <= heap[i]:\n",
    "                heap[i] -= r\n",
    "                last_player = 'human'\n",
    "                break\n",
    "            print(\" Invalid move—try again.\")\n",
    "        current_player = 'computer'\n",
    "\n",
    "    else:  # computer’s turn\n",
    "        i, r = get_computer_move(heap)\n",
    "        heap[i] -= r\n",
    "        last_player = 'computer'\n",
    "        print(f\"\\nComputer removes {r} from pile {i}. New piles:\", heap)\n",
    "        current_player = 'human'\n",
    "\n",
    "# Game over\n",
    "print(\"\\nGame over!\")\n",
    "if last_player == 'human':\n",
    "    print(\"🎉 You took the last stone — you win!\")\n",
    "else:\n",
    "    print(\"💻 Computer took the last stone — you lose!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "9595a39a-3dad-4211-b000-effa3e2a09b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from textblob import TextBlob\n",
    "\n",
    "# nltk.download('punkt_tab')\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "with open('text.txt','r') as file:\n",
    "    text = file.read()\n",
    "\n",
    "text = re.sub(r'[^a-zA-Z\\s]','',text)\n",
    "text = re.sub(r'\\s+',' ',text)\n",
    "text = text.lower()\n",
    "\n",
    "tokenized = word_tokenize(text)\n",
    "\n",
    "filtered = [w for w in tokenized if w not in stopwords.words('english')]\n",
    "\n",
    "correct = [str(TextBlob(word).correct()) for word in filtered]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "9ef10a06-8734-4667-983b-2d2a5a5abc99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['artificial', 'intelligence', 'ai', 'transforming', 'various', 'industries', 'automatic', 'tasks', 'enhancing', 'decisionmaking', 'improving', 'efficiency', 'healthcare', 'finance', 'ai', 'applications', 'becoming', 'increasingly', 'prevalent', 'healthcare', 'ai', 'assist', 'diagnosing', 'diseases', 'predictions', 'patient', 'outcome', 'personalizing', 'treatment', 'plans', 'financial', 'institutions', 'beverage', 'ai', 'fraud', 'detection', 'risk', 'assessment', 'algorithmic', 'trading', 'automobile', 'sector', 'ai', 'powers', 'autonomous', 'vehicles', 'enabling', 'navigable', 'make', 'decisions', 'realize', 'retail', 'business', 'use', 'ai', 'analyze', 'consumer', 'behavior', 'optimism', 'inventor', 'provide', 'personalized', 'recommendations', 'moreover', 'driven', 'whatnots', 'enhance', 'customer', 'service', 'providing', 'instant', 'responses', 'handling', 'routine', 'inquiries', 'despite', 'benefits', 'ai', 'also', 'raises', 'ethical', 'concerns', 'data', 'privacy', 'job', 'displacement', 'decisionmaking', 'transparent', 'ensuring', 'responsible', 'ai', 'development', 'involves', 'addressing', 'challenges', 'regulations', 'ethical', 'guideline', 'continuous', 'monitoring', 'ai', 'continues', 'evolve', 'integration', 'daily', 'life', 'likely', 'become', 'fearless', 'influencing', 'work', 'communicate', 'solve', 'complex', 'problems']\n"
     ]
    }
   ],
   "source": [
    "print(correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "b189ba89-1c40-4210-b5d2-a6518f3a9c2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Vrushant\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "b5001053-68be-46b3-936e-7637f7915337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['artifici', 'intellig', 'ai', 'transform', 'variou', 'industri', 'automat', 'task', 'enhanc', 'decisionmak', 'improv', 'effici', 'healthcar', 'financ', 'ai', 'applic', 'becom', 'increasingli', 'preval', 'healthcar', 'ai', 'assist', 'diagnos', 'diseas', 'predict', 'patient', 'outcom', 'person', 'treatment', 'plan', 'financi', 'institut', 'beverag', 'ai', 'fraud', 'detect', 'risk', 'assess', 'algorithm', 'trade', 'automobil', 'sector', 'ai', 'power', 'autonom', 'vehicl', 'enabl', 'navig', 'make', 'decis', 'realiz', 'retail', 'busi', 'use', 'ai', 'analyz', 'consum', 'behavior', 'optim', 'inventor', 'provid', 'person', 'recommend', 'moreov', 'driven', 'whatnot', 'enhanc', 'custom', 'servic', 'provid', 'instant', 'respons', 'handl', 'routin', 'inquiri', 'despit', 'benefit', 'ai', 'also', 'rais', 'ethic', 'concern', 'data', 'privaci', 'job', 'displac', 'decisionmak', 'transpar', 'ensur', 'respons', 'ai', 'develop', 'involv', 'address', 'challeng', 'regul', 'ethic', 'guidelin', 'continu', 'monitor', 'ai', 'continu', 'evolv', 'integr', 'daili', 'life', 'like', 'becom', 'fearless', 'influenc', 'work', 'commun', 'solv', 'complex', 'problem']\n"
     ]
    }
   ],
   "source": [
    "stemmer = PorterStemmer()\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "stemmed = [stemmer.stem(w) for w in correct]\n",
    "lemmatized = [lemma.lemmatize(w) for w in correct]\n",
    "\n",
    "print(stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "d2ca9135-bb83-4f4f-addc-00d867ff5369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['artificial', 'intelligence', 'ai', 'transforming', 'various', 'industry', 'automatic', 'task', 'enhancing', 'decisionmaking', 'improving', 'efficiency', 'healthcare', 'finance', 'ai', 'application', 'becoming', 'increasingly', 'prevalent', 'healthcare', 'ai', 'assist', 'diagnosing', 'disease', 'prediction', 'patient', 'outcome', 'personalizing', 'treatment', 'plan', 'financial', 'institution', 'beverage', 'ai', 'fraud', 'detection', 'risk', 'assessment', 'algorithmic', 'trading', 'automobile', 'sector', 'ai', 'power', 'autonomous', 'vehicle', 'enabling', 'navigable', 'make', 'decision', 'realize', 'retail', 'business', 'use', 'ai', 'analyze', 'consumer', 'behavior', 'optimism', 'inventor', 'provide', 'personalized', 'recommendation', 'moreover', 'driven', 'whatnot', 'enhance', 'customer', 'service', 'providing', 'instant', 'response', 'handling', 'routine', 'inquiry', 'despite', 'benefit', 'ai', 'also', 'raise', 'ethical', 'concern', 'data', 'privacy', 'job', 'displacement', 'decisionmaking', 'transparent', 'ensuring', 'responsible', 'ai', 'development', 'involves', 'addressing', 'challenge', 'regulation', 'ethical', 'guideline', 'continuous', 'monitoring', 'ai', 'continues', 'evolve', 'integration', 'daily', 'life', 'likely', 'become', 'fearless', 'influencing', 'work', 'communicate', 'solve', 'complex', 'problem']\n"
     ]
    }
   ],
   "source": [
    "print(lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "fb8605db-f96a-423a-bc8c-33ffe3983405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['artificial intelligence ai', 'intelligence ai transforming', 'ai transforming various', 'transforming various industry', 'various industry automatic', 'industry automatic task', 'automatic task enhancing', 'task enhancing decisionmaking', 'enhancing decisionmaking improving', 'decisionmaking improving efficiency', 'improving efficiency healthcare', 'efficiency healthcare finance', 'healthcare finance ai', 'finance ai application', 'ai application becoming', 'application becoming increasingly', 'becoming increasingly prevalent', 'increasingly prevalent healthcare', 'prevalent healthcare ai', 'healthcare ai assist', 'ai assist diagnosing', 'assist diagnosing disease', 'diagnosing disease prediction', 'disease prediction patient', 'prediction patient outcome', 'patient outcome personalizing', 'outcome personalizing treatment', 'personalizing treatment plan', 'treatment plan financial', 'plan financial institution', 'financial institution beverage', 'institution beverage ai', 'beverage ai fraud', 'ai fraud detection', 'fraud detection risk', 'detection risk assessment', 'risk assessment algorithmic', 'assessment algorithmic trading', 'algorithmic trading automobile', 'trading automobile sector', 'automobile sector ai', 'sector ai power', 'ai power autonomous', 'power autonomous vehicle', 'autonomous vehicle enabling', 'vehicle enabling navigable', 'enabling navigable make', 'navigable make decision', 'make decision realize', 'decision realize retail', 'realize retail business', 'retail business use', 'business use ai', 'use ai analyze', 'ai analyze consumer', 'analyze consumer behavior', 'consumer behavior optimism', 'behavior optimism inventor', 'optimism inventor provide', 'inventor provide personalized', 'provide personalized recommendation', 'personalized recommendation moreover', 'recommendation moreover driven', 'moreover driven whatnot', 'driven whatnot enhance', 'whatnot enhance customer', 'enhance customer service', 'customer service providing', 'service providing instant', 'providing instant response', 'instant response handling', 'response handling routine', 'handling routine inquiry', 'routine inquiry despite', 'inquiry despite benefit', 'despite benefit ai', 'benefit ai also', 'ai also raise', 'also raise ethical', 'raise ethical concern', 'ethical concern data', 'concern data privacy', 'data privacy job', 'privacy job displacement', 'job displacement decisionmaking', 'displacement decisionmaking transparent', 'decisionmaking transparent ensuring', 'transparent ensuring responsible', 'ensuring responsible ai', 'responsible ai development', 'ai development involves', 'development involves addressing', 'involves addressing challenge', 'addressing challenge regulation', 'challenge regulation ethical', 'regulation ethical guideline', 'ethical guideline continuous', 'guideline continuous monitoring', 'continuous monitoring ai', 'monitoring ai continues', 'ai continues evolve', 'continues evolve integration', 'evolve integration daily', 'integration daily life', 'daily life likely', 'life likely become', 'likely become fearless', 'become fearless influencing', 'fearless influencing work', 'influencing work communicate', 'work communicate solve', 'communicate solve complex', 'solve complex problem']\n"
     ]
    }
   ],
   "source": [
    "three_grams = [' '.join(lemmatized[i:i+3]) for i in range(len(lemmatized)-2)]\n",
    "print(three_grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "b1315d11-308f-488a-bad2-5e6f0210b455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words:  ['an', 'is', 'vrushant', 'boy', 'awesome', 'this', 'mukherjee']\n",
      "One Hot Encoding: \n",
      "an: [1. 0. 0. 0. 0. 0. 0.]\n",
      "is: [0. 0. 0. 1. 0. 0. 0.]\n",
      "vrushant: [0. 0. 0. 0. 0. 0. 1.]\n",
      "boy: [0. 0. 1. 0. 0. 0. 0.]\n",
      "awesome: [0. 1. 0. 0. 0. 0. 0.]\n",
      "this: [0. 0. 0. 0. 0. 1. 0.]\n",
      "mukherjee: [0. 0. 0. 0. 1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "f1 = \"This is Vrushant Mukherjee\"\n",
    "f2 = \"Vrushant Mukherjee is Awesome\"\n",
    "f3 = \"This is an awesome boy\"\n",
    "\n",
    "corpus = f1+\" \"+f2+\" \"+f3\n",
    "\n",
    "words = list(set(re.sub(r'[^a-zA-Z\\s]','',corpus.lower()).split()))\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "encoded = encoder.fit_transform([[w] for w in words])\n",
    "\n",
    "print(\"Words: \", words)\n",
    "print(\"One Hot Encoding: \")\n",
    "for i, word in enumerate(words):\n",
    "    print(f\"{word}: {encoded[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "930da5d8-062c-46ff-9a34-8e8c132e80dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a bag of words for sentence this is boy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "17d91c1e-34ca-4448-8983-32ac3976ba29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of Words Matrix:\n",
      "[[ 0  1  1  0  0  0  1  6  0  1  0  1  0  0  0  0  0  1  1  0  0  1  1  0\n",
      "   1  0  1  1  0  1  0  1  0  0  3  1  0  0  1  0  1  0  1  0  0  0  0  0\n",
      "   0  0  1  0  0  0  0  1  1  1  1  0  1  0  1  0  0  0  1  0  1  1  1  0\n",
      "   1  1  0  1  0  1  0  0  1  0  1  0  1  1  1  1  0  4  0  0  0  1  0  0\n",
      "   0  0  0  1  1  0  0  1  0  1  0  1  1  1  2  1  0  0  0  1  1  1  0  1\n",
      "   3  1  1  0  0  0  1  0  0  2  1  0  0  1  3  1  1  1  0  0  0  0  0  0\n",
      "   1  1  0  0  0  0  1  0  1  1  0  1  0  1  0  3  1  0  0  0  1  1  1  0\n",
      "   0  0  1  0  4  1  0  0  0  0  1  0  1  1  0  1  0  1  1  0  1  1  0  1\n",
      "   1  0  0  1  0  1  0  0  0  1  0  0  2  0  0  0  0  0  0  0  1  1  1  0\n",
      "   1  0  0  0  0  0  1  0  1  1  0  1  0  0  0  0  1  0  0  1  1  0  1  1\n",
      "   0  0  3 11  0  1  1  0  0  0  0  3  1  0  0  0  0  1  0  0  1  1  1  0\n",
      "   0  1  0  0  1  0  0  0  0  1]\n",
      " [ 0  0  0  0  1  0  0  4  2  0  0  2  1  1  1  1  0  0  0  0  0  2  0  1\n",
      "   0  1  0  0  1  0  1  1  1  1  0  0  0  1  1  0  0  0  0  0  1  0  0  0\n",
      "   0  1  0  0  1  1  1  0  0  0  0  1  0  1  1  0  0  0  0  1  0  0  0  1\n",
      "   0  0  0  0  0  0  1  1  0  0  0  1  0  1  0  0  0  3  0  1  0  0  2  1\n",
      "   0  1  1  0  0  0  0  0  0  0  0  0  1  1  0  1  2  1  0  0  0  0  1  5\n",
      "   0  0  0  0  0  1  0  0  0  0  0  1  0  0  2  0  2  0  1  0  0  1  0  1\n",
      "   0  0  1  3  1  0  0  1  0  0  0  0  0  0  0  0  0  1  1  0  0  1  0  0\n",
      "   0  1  0  0  7  0  0  0  1  0  0  1  0  0  1  0  0  0  0  1  0  0  1  0\n",
      "   0  1  0  0  1  1  2  1  0  0  0  1  0  1  2  0  0  1  1  1  0  0  1  0\n",
      "   0  0  1  0  1  3  0  0  0  0  0  0  1  0  1  1  1  1  1  0  0  1  0  0\n",
      "   0  1  1 15  1  1  0  1  0  1  0  3  0  0  1  0  1  0  0  0  1  0  0  1\n",
      "   0  0  1  1  0  0  1  0  0  0]\n",
      " [ 1  1  1  1  0  1  1  5  0  0  1  0  1  0  0  0  1  0  0  2  1  2  0  0\n",
      "   0  0  0  0  0  0  0  0  1  0  0  0  1  0  0  1  0  1  0  1  0  1  1  1\n",
      "   1  0  0  1  0  0  0  0  1  0  0  0  0  0  1  1  1  1  0  0  0  0  0  0\n",
      "   0  1  1  0  1  0  0  0  0  1  0  0  1  0  0  1  1  5  1  0  1  1  2  0\n",
      "   1  0  0  1  0  1  1  0  1  0  1  0  0  1  0  0  0  0  1  0  0  1  0  0\n",
      "   0  0  0  1  1  0  0  1  1  1  0  0  1  0  2  1  2  0  0  1  1  0  1  0\n",
      "   0  0  0  0  0  1  0  0  0  0  1  0  3  0  1  0  0  0  0  1  0  1  1  2\n",
      "   1  0  0  1  5  0  1  1  0  1  0  0  0  1  0  0  1  0  0  0  0  1  0  0\n",
      "   0  0  1  0  0  0  0  0  1  0  1  0  3  0  0  1  1  0  0  0  1  1  0  1\n",
      "   0  1  0  1  1  0  0  1  0  0  1  0  0  1  0  0  0  0  0  0  0  0  0  0\n",
      "   1  0  3 13  1  1  0  0  1  0  1  3  0  1  0  1  0  1  1  1  1  0  1  0\n",
      "   1  0  0  0  1  1  1  1  1  0]]\n",
      "Vocabulary:\n",
      "['about' 'action' 'adds' 'aesthetic' 'allows' 'also' 'an' 'and' 'andy'\n",
      " 'are' 'arts' 'as' 'atmosphere' 'audience' 'banker' 'beauty' 'became'\n",
      " 'bending' 'between' 'bullet' 'but' 'by' 'can' 'captures' 'cast'\n",
      " 'cemented' 'challenges' 'chance' 'character' 'christopher' 'chronicles'\n",
      " 'cinematic' 'cinematography' 'classic' 'cobb' 'combines' 'combining'\n",
      " 'commit' 'compelling' 'complements' 'complex' 'computer' 'concept'\n",
      " 'consciousness' 'contrasted' 'control' 'controlled' 'countless' 'creates'\n",
      " 'crime' 'criminal' 'cyberpunk' 'darabont' 'darkest' 'deep' 'delivers'\n",
      " 'delves' 'demons' 'depth' 'development' 'dicaprio' 'didn' 'directed'\n",
      " 'discovers' 'discussions' 'dodging' 'dom' 'drama' 'dream' 'dreams'\n",
      " 'dreamscapes' 'dufresne' 'during' 'effects' 'elements' 'ellen' 'embodies'\n",
      " 'emotion' 'emotional' 'endurance' 'enhances' 'entertains' 'erased' 'even'\n",
      " 'experience' 'explores' 'extracting' 'fiction' 'figure' 'film' 'films'\n",
      " 'find' 'fishburne' 'follows' 'for' 'frank' 'free' 'freeman' 'friendship'\n",
      " 'from' 'gordon' 'gravitas' 'groundbreaking' 'guilt' 'hacker' 'hans' 'has'\n",
      " 'haunting' 'have' 'he' 'his' 'history' 'hope' 'human' 'iconic' 'idea'\n",
      " 'if' 'immersive' 'impression' 'in' 'inception' 'inceptionâ' 'including'\n",
      " 'individual' 'influenced' 'inspiring' 'intellect' 'intelligent' 'intense'\n",
      " 'into' 'intricate' 'invested' 'invites' 'involving' 'is' 'it' 'its'\n",
      " 'joseph' 'journeys' 'keanu' 'knows' 'lasting' 'laurence' 'leaves'\n",
      " 'leonardo' 'levitt' 'liberation' 'life' 'light' 'machines' 'make'\n",
      " 'making' 'man' 'manipulation' 'martial' 'masterpiece' 'matrix' 'memory'\n",
      " 'messianic' 'mind' 'mission' 'moments' 'morgan' 'morpheus' 'multiple'\n",
      " 'narrative' 'nature' 'neo' 'new' 'newman' 'nolan' 'not' 'of' 'offered'\n",
      " 'on' 'only' 'oppressive' 'ordinary' 'overall' 'pacing' 'page'\n",
      " 'particularly' 'penitentiary' 'people' 'perceptions' 'perform'\n",
      " 'performance' 'performances' 'personal' 'philosophical' 'places'\n",
      " 'planting' 'plot' 'poignant' 'portrayal' 'portraying' 'portrays' 'power'\n",
      " 'prison' 'provides' 'provoking' 'question' 'questions' 'quiet' 'reality'\n",
      " 'red' 'redemption' 'reeves' 'reflect' 'resilience' 'robbins' 'role'\n",
      " 'scenes' 'science' 'score' 'search' 'secrets' 'seminal' 'sentenced'\n",
      " 'sequences' 'set' 'shawshank' 'shifting' 'simulated' 'skilled' 'someone'\n",
      " 'soundtrack' 'specializes' 'spirit' 'standards' 'state' 'status' 'story'\n",
      " 'storytelling' 'strength' 'stunning' 'subconscious' 'subtly'\n",
      " 'successfully' 'supporting' 'technology' 'testament' 'that' 'the' 'their'\n",
      " 'themes' 'thief' 'thomas' 'thought' 'tim' 'time' 'to' 'torn'\n",
      " 'transformation' 'transformative' 'truth' 'underscores' 'undertones'\n",
      " 'unique' 'use' 'viewers' 'viewings' 'visual' 'voice' 'wachowskis'\n",
      " 'warrants' 'weight' 'while' 'who' 'will' 'with' 'work' 'world' 'zimmer']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "documents = []\n",
    "for file_name in [\"review1.txt\", \"review2.txt\", \"review3.txt\"]:\n",
    "    with open(file_name, \"r\") as f:\n",
    "        documents.append(f.read())\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(documents)\n",
    "\n",
    "print(\"Bag of Words Matrix:\")\n",
    "print(X.toarray())\n",
    "print(\"Vocabulary:\")\n",
    "print(vectorizer.get_feature_names_out())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "42556b25-f68a-4bf7-8380-04f543822dc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the number of Inputs: 4\n",
      "Enter the number of neurons in Hidden Layer 1:  3\n",
      "Enter the number of neurons in Hidden Layer 2:  3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input X: [[0 0 1 1]]\n",
      "Output Y: [[4.81711587]]\n",
      "\n",
      "W1: [[0.55511433 0.50373966 0.86108869]\n",
      " [0.85619178 0.82269051 0.35018293]\n",
      " [0.99648524 0.5146342  0.98554348]\n",
      " [0.62402039 0.25652532 0.29334421]] \n",
      "W2: [[0.56821682 0.57376899 0.95537793]\n",
      " [0.63671453 0.32277415 0.22130038]\n",
      " [0.49967785 0.6718806  0.69596885]] \n",
      "W3: [[0.80904994]\n",
      " [0.35436803]\n",
      " [0.05542408]]\n",
      "\n",
      "b1: [[0.87515627 0.56996159 0.61573701]] \n",
      "b2: [[0.40563864 0.4784542  0.00687497]] \n",
      "b3: [[0.38131676]]\n",
      "0.7871642602005242\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Random input: N\n",
    "N = int(input(\"Enter the number of Inputs:\"))\n",
    "hidden1_size = int(input(\"Enter the number of neurons in Hidden Layer 1: \"))\n",
    "hidden2_size = int(input(\"Enter the number of neurons in Hidden Layer 2: \"))\n",
    "output_size = 1\n",
    "\n",
    "# Input Layer (Random binary: 0 or 1)\n",
    "X = np.random.randint(0, 2, (1, N))\n",
    "\n",
    "# Random Weights & Biases\n",
    "W1 = np.random.rand(N, hidden1_size)\n",
    "b1 = np.random.rand(1, hidden1_size)\n",
    "\n",
    "W2 = np.random.rand(hidden1_size, hidden2_size)\n",
    "b2 = np.random.rand(1, hidden2_size)\n",
    "\n",
    "W3 = np.random.rand(hidden2_size, output_size)\n",
    "b3 = np.random.rand(1, output_size)\n",
    "\n",
    "# Forward pass\n",
    "Z1 = np.dot(X, W1) + b1\n",
    "Z2 = np.dot(Z1, W2) + b2\n",
    "Y = np.dot(Z2, W3) + b3\n",
    "\n",
    "# Output results\n",
    "print(\"Input X:\", X)\n",
    "print(\"Output Y:\", Y)\n",
    "print(\"\\nW1:\", W1, \"\\nW2:\", W2, \"\\nW3:\", W3)\n",
    "print(\"\\nb1:\", b1, \"\\nb2:\", b2, \"\\nb3:\", b3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "ec0e6f36-5eca-4273-8bc8-52ab0ca783fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input X: [[1 1 1 1]]\n",
      "Output Y: [[3.41097355 3.91335231]]\n",
      "\n",
      "W1: [[0.78931336 0.984268   0.12968476]\n",
      " [0.52940068 0.41570949 0.44446496]\n",
      " [0.37781971 0.95924876 0.73729899]\n",
      " [0.01759398 0.39166617 0.60207076]] \n",
      "W2: [[0.26265355 0.61147876]\n",
      " [0.01657269 0.37052275]\n",
      " [0.71109702 0.37823642]]\n",
      "\n",
      "b1: [[0.34838396 0.07535154 0.68578105]] \n",
      "b2: [[0.97405422 0.62183241]]\n",
      "[0.96804573 0.98041769]\n"
     ]
    }
   ],
   "source": [
    "# Input: 4 binary inputs\n",
    "X = np.random.randint(0, 2, (1, 4))\n",
    "\n",
    "# Weights and Biases\n",
    "W1 = np.random.rand(4, 3)\n",
    "b1 = np.random.rand(1, 3)\n",
    "\n",
    "W2 = np.random.rand(3, 2)\n",
    "b2 = np.random.rand(1, 2)\n",
    "\n",
    "# Forward pass\n",
    "Z1 = np.dot(X, W1) + b1\n",
    "Y = np.dot(Z1, W2) + b2\n",
    "\n",
    "# Output results\n",
    "print(\"Input X:\", X)\n",
    "print(\"Output Y:\", Y)\n",
    "print(\"\\nW1:\", W1, \"\\nW2:\", W2)\n",
    "print(\"\\nb1:\", b1, \"\\nb2:\", b2)\n",
    "\n",
    "def sigmoid(Y):\n",
    "    return 1 / (1+np.exp(-Y))\n",
    "\n",
    "\n",
    "for i in Y:\n",
    "    bi = sigmoid(i)\n",
    "    print(bi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "408e3634-3b97-4e90-b03f-ef2828c7686a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Output after Training: [[0.50276506]\n",
      " [0.50213261]\n",
      " [0.50277938]\n",
      " [0.50190748]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_deriv(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "# Dataset\n",
    "X = np.array([[0, 1], [1, 0], [1, 1], [0, 0]])\n",
    "Y = np.array([[1], [1], [0], [0]])\n",
    "\n",
    "# Weights Initialization\n",
    "W1 = np.random.rand(2, 4)\n",
    "W2 = np.random.rand(4, 3)\n",
    "W3 = np.random.rand(3, 1)\n",
    "\n",
    "# Training Loop\n",
    "for step in range(1000):\n",
    "    # Forward pass\n",
    "    Z1 = sigmoid(np.dot(X, W1))\n",
    "    Z2 = sigmoid(np.dot(Z1, W2))\n",
    "    output = sigmoid(np.dot(Z2, W3))\n",
    "\n",
    "    # Error\n",
    "    error = Y - output\n",
    "\n",
    "    # Backward pass\n",
    "    d_output = error * sigmoid_deriv(output)\n",
    "    d_W3 = np.dot(Z2.T, d_output)\n",
    "\n",
    "    d_Z2 = np.dot(d_output, W3.T) * sigmoid_deriv(Z2)\n",
    "    d_W2 = np.dot(Z1.T, d_Z2)\n",
    "\n",
    "    d_Z1 = np.dot(d_Z2, W2.T) * sigmoid_deriv(Z1)\n",
    "    d_W1 = np.dot(X.T, d_Z1)\n",
    "\n",
    "    # Update Weights\n",
    "    W1 += d_W1\n",
    "    W2 += d_W2\n",
    "    W3 += d_W3\n",
    "\n",
    "print(\"Final Output after Training:\", output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "af984fc7-f377-4504-8095-5ef7ab9b42d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Output: [[0.50000042]\n",
      " [0.49999947]\n",
      " [0.49999989]\n",
      " [0.5       ]]\n"
     ]
    }
   ],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_deriv(x):\n",
    "    return (x > 0).astype(float)\n",
    "\n",
    "X = np.array([[0, 1], [1, 0], [1, 1], [0, 0]])\n",
    "Y = np.array([[1], [1], [0], [0]])\n",
    "\n",
    "W1 = np.random.rand(2, 4)\n",
    "W2 = np.random.rand(4, 3)\n",
    "W3 = np.random.rand(3, 1)\n",
    "\n",
    "for step in range(1000):\n",
    "    Z1 = relu(np.dot(X, W1))\n",
    "    Z2 = relu(np.dot(Z1, W2))\n",
    "    output = sigmoid(np.dot(Z2, W3))\n",
    "\n",
    "    error = Y - output\n",
    "    d_output = error * sigmoid_deriv(output)\n",
    "\n",
    "    d_W3 = np.dot(Z2.T, d_output)\n",
    "    d_Z2 = np.dot(d_output, W3.T) * relu_deriv(Z2)\n",
    "    d_W2 = np.dot(Z1.T, d_Z2)\n",
    "    d_Z1 = np.dot(d_Z2, W2.T) * relu_deriv(Z1)\n",
    "    d_W1 = np.dot(X.T, d_Z1)\n",
    "\n",
    "    W1 += d_W1\n",
    "    W2 += d_W2\n",
    "    W3 += d_W3\n",
    "\n",
    "print(\"Final Output:\", output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ab9e42-c4df-405c-8bf6-6dca06c5c9ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd15afe-74da-4d4a-a543-12b6f21934e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c67cbe9-f312-42e5-bc78-29ca51a46e3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
