{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e5871e-a00d-42a4-8668-5600fedd3962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ESE Practical Examination\n",
    "# Author: Vrushant Mukherjee\n",
    "# All 26 Experiments Combined\n",
    "\n",
    "# ================= Experiment 1 =================\n",
    "# Recursive DFS from CSV\n",
    "import pandas as pd\n",
    "import heapq\n",
    "import csv\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from textblob import TextBlob\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def dfs_recursive(graph, node, visited):\n",
    "    if node not in visited:\n",
    "        print(node, end=\" \")\n",
    "        visited.add(node)\n",
    "        for neighbor in graph[node]:\n",
    "            dfs_recursive(graph, neighbor, visited)\n",
    "\n",
    "# ================= Experiment 2 =================\n",
    "# Non-Recursive DFS\n",
    "\n",
    "def dfs_non_recursive(graph, start):\n",
    "    visited = set()\n",
    "    stack = [start]\n",
    "    while stack:\n",
    "        node = stack.pop()\n",
    "        if node not in visited:\n",
    "            print(node, end=\" \")\n",
    "            visited.add(node)\n",
    "            stack.extend(reversed(graph[node]))\n",
    "\n",
    "# ================= Experiment 3 =================\n",
    "# Breadth First Search (BFS)\n",
    "\n",
    "def bfs(graph, start):\n",
    "    visited = set()\n",
    "    queue = deque([start])\n",
    "    while queue:\n",
    "        node = queue.popleft()\n",
    "        if node not in visited:\n",
    "            print(node, end=\" \")\n",
    "            visited.add(node)\n",
    "            queue.extend(graph[node])\n",
    "\n",
    "# ================= Experiment 4-7 =================\n",
    "# Best First Search Variants\n",
    "\n",
    "def best_first_search(graph, heuristics, start, goal):\n",
    "    visited = set()\n",
    "    queue = [(heuristics[start], start)]\n",
    "    while queue:\n",
    "        _, node = heapq.heappop(queue)\n",
    "        if node not in visited:\n",
    "            print(node, end=\" \")\n",
    "            visited.add(node)\n",
    "            if node == goal:\n",
    "                break\n",
    "            for neighbor in graph[node]:\n",
    "                heapq.heappush(queue, (heuristics[neighbor], neighbor))\n",
    "\n",
    "def best_first_search_weighted(graph, heuristics, start, goal):\n",
    "    visited = set()\n",
    "    queue = [(heuristics[start], start)]\n",
    "    while queue:\n",
    "        _, node = heapq.heappop(queue)\n",
    "        if node not in visited:\n",
    "            print(node, end=\" \")\n",
    "            visited.add(node)\n",
    "            if node == goal:\n",
    "                break\n",
    "            for neighbor, weight in graph[node]:\n",
    "                heapq.heappush(queue, (heuristics[neighbor], neighbor))\n",
    "\n",
    "# ================= Experiment 8-11 =================\n",
    "# A* Algorithm Variants\n",
    "\n",
    "def read_graph_from_csv(file, undirected=False):\n",
    "    graph = {}\n",
    "    heuristics = {}\n",
    "    with open(file, 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        next(reader)\n",
    "        for row in reader:\n",
    "            src, dest, weight, heuristic = row\n",
    "            weight = int(weight)\n",
    "            heuristic = int(heuristic)\n",
    "            graph.setdefault(src, []).append((dest, weight))\n",
    "            heuristics[src] = heuristic\n",
    "            heuristics[dest] = heuristics.get(dest, 0)\n",
    "            if undirected:\n",
    "                graph.setdefault(dest, []).append((src, weight))\n",
    "    return graph, heuristics\n",
    "\n",
    "def a_star(graph, heuristics, start, goal):\n",
    "    open_set = [(heuristics[start], 0, start)]\n",
    "    visited = set()\n",
    "    while open_set:\n",
    "        f, cost, node = heapq.heappop(open_set)\n",
    "        if node == goal:\n",
    "            print(f\"Reached {node} with cost {cost}\")\n",
    "            return\n",
    "        if node not in visited:\n",
    "            visited.add(node)\n",
    "            for neighbor, weight in graph.get(node, []):\n",
    "                heapq.heappush(open_set, (cost + weight + heuristics[neighbor], cost + weight, neighbor))\n",
    "\n",
    "# ================= Experiment 12-14 =================\n",
    "# Fuzzy Sets\n",
    "\n",
    "def fuzzy_union(*sets):\n",
    "    return {k: max(s.get(k, 0) for s in sets) for k in sets[0]}\n",
    "\n",
    "def fuzzy_intersection(*sets):\n",
    "    return {k: min(s.get(k, 0) for s in sets) for k in sets[0]}\n",
    "\n",
    "def fuzzy_complement(s):\n",
    "    return {k: 1 - v for k, v in s.items()}\n",
    "\n",
    "# ================= Experiment 15-16 =================\n",
    "# Nim Game with Min-Max\n",
    "\n",
    "def nim_sum(heap):\n",
    "    result = 0\n",
    "    for pile in heap:\n",
    "        result ^= pile\n",
    "    return result\n",
    "\n",
    "# ================= Experiment 17-21 =================\n",
    "# MLP Networks\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_deriv(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_deriv(x):\n",
    "    return (x > 0).astype(float)\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_deriv(x):\n",
    "    return 1 - np.tanh(x)**2\n",
    "\n",
    "# ================= Experiment 22-26 =================\n",
    "# NLP Preprocessing\n",
    "\n",
    "def text_preprocessing(text):\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = text.lower()\n",
    "    tokens = word_tokenize(text)\n",
    "    filtered = [w for w in tokens if w not in stopwords.words('english')]\n",
    "    corrected = [str(TextBlob(word).correct()) for word in filtered]\n",
    "    return corrected\n",
    "\n",
    "def create_bag_of_words(documents):\n",
    "    vectorizer = CountVectorizer()\n",
    "    X = vectorizer.fit_transform(documents)\n",
    "    return X.toarray(), vectorizer.get_feature_names_out()\n",
    "\n",
    "def create_tfidf_matrix(documents):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X = vectorizer.fit_transform(documents)\n",
    "    return X.toarray(), vectorizer.get_feature_names_out()\n",
    "\n",
    "def one_hot_encode_words(words):\n",
    "    encoder = OneHotEncoder(sparse=False)\n",
    "    encoded = encoder.fit_transform([[w] for w in words])\n",
    "    return encoded\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
